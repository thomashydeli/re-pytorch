{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import DataLoader\n",
    "from module.data_processes import get_data\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer=SummaryWriter(log_dir='runs/data_10_perc-effnetb0-5_epochs-lr_0_001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir, test_dir=get_data(\n",
    "    data_path='data/pizza_steak_sushi',\n",
    "    data_url='https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip',\n",
    "    unzip=True,\n",
    ")\n",
    "\n",
    "# auto creation of transformations\n",
    "weights=torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
    "auto_transforms=weights.transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=32\n",
    "DEVICE='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataset=datasets.ImageFolder(\n",
    "    root=train_dir,\n",
    "    transform=auto_transforms,\n",
    ") # creating training dataset\n",
    "\n",
    "train_dataloader=DataLoader(\n",
    "    trainDataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    ") # creating training dataloader\n",
    "\n",
    "# getting the name and indexer of classes:\n",
    "class_name=trainDataset.classes\n",
    "class_dict=trainDataset.class_to_idx\n",
    "\n",
    "# setting up the testing dataset\n",
    "testDataset=datasets.ImageFolder(\n",
    "    root=test_dir,\n",
    "    transform=auto_transforms,\n",
    ") # creating testing dataset\n",
    "\n",
    "# creating the testing dataloader\n",
    "test_dataloader=DataLoader(\n",
    "    testDataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    ") # creating testing dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=torchvision.models.efficientnet_b0(weights=weights).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # getting a summary of our model with torchinfo.summary()\n",
    "# summary(\n",
    "#     model, \n",
    "#     input_size=(1,3,224,224),\n",
    "#     col_names=['input_size', 'output_size', 'num_params', 'trainable'],\n",
    "#     col_width=15,\n",
    "#     row_settings=['var_names']\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.features.parameters():\n",
    "    param.requires_grad=False\n",
    "\n",
    "# Update the classifier head of our model to suit the problem\n",
    "model.classifier=nn.Sequential(\n",
    "    nn.Dropout(p=0.2, inplace=True),\n",
    "    nn.Linear(\n",
    "        in_features=1280,\n",
    "        out_features=len(class_name)\n",
    "    )\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn=torch.nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(\n",
    "    dataLoader,\n",
    "    device='cpu',\n",
    "    infer=False,\n",
    "):\n",
    "    if infer:\n",
    "        model.eval()\n",
    "    else:\n",
    "        model.train()\n",
    "\n",
    "    currentLoss, currentMetric=0, 0\n",
    "    for batch, (X,y) in enumerate(dataLoader):\n",
    "        X,y=X.to(device), y.to(device)\n",
    "        y_pred=model(X)\n",
    "        loss=loss_fn(y_pred, y)\n",
    "        currentLoss += loss.item()\n",
    "\n",
    "        if not infer:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        y_pred_output=torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "        currentMetric+=(y_pred_output==y).sum().item()/len(y_pred)\n",
    "\n",
    "    currentLoss=currentLoss/len(dataLoader)\n",
    "    currentMetric=currentMetric/len(dataLoader)\n",
    "    return currentLoss, currentMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a100e3ff192459d864a3d3c7d15b0c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:64] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    }
   ],
   "source": [
    "epochs=5\n",
    "for epoch in tqdm(range(epochs), total=epochs):\n",
    "    train_loss, train_acc=step(\n",
    "        train_dataloader,\n",
    "    )\n",
    "\n",
    "    test_loss, test_acc=step(\n",
    "        test_dataloader,\n",
    "        infer=True,\n",
    "    )\n",
    "    # print(f\"Epoch: {epoch} | train loss: {train_loss} | train_acc: {train_acc}\")\n",
    "    # print(f\"test loss: {test_loss} | test_acc: {test_acc}\")\n",
    "\n",
    "    writer.add_scalars(\n",
    "        main_tag='Loss',\n",
    "        tag_scalar_dict={\n",
    "            'train_loss':train_loss,\n",
    "            'test_loss':test_loss,\n",
    "        },\n",
    "        global_step=epoch\n",
    "    )\n",
    "\n",
    "    writer.add_scalars(\n",
    "        main_tag='Accuracy',\n",
    "        tag_scalar_dict={\n",
    "            'train_acc':train_acc,\n",
    "            'test_acc':test_acc,\n",
    "        },\n",
    "        global_step=epoch\n",
    "    )\n",
    "\n",
    "    writer.add_graph(\n",
    "        model=model,\n",
    "        input_to_model=torch.randn(32,3,224,224).to(DEVICE)\n",
    "    )\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
